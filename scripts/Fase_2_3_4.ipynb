{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c75ac994-4130-4bbe-a3d6-f3b40267a3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cassandra-driver\n",
      "  Downloading cassandra_driver-3.29.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (6.9 kB)\n",
      "Collecting geomet>=1.1 (from cassandra-driver)\n",
      "  Downloading geomet-1.1.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from geomet>=1.1->cassandra-driver) (8.1.7)\n",
      "Downloading cassandra_driver-3.29.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (374 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.3/374.3 kB\u001b[0m \u001b[31m654.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading geomet-1.1.0-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: geomet, cassandra-driver\n",
      "Successfully installed cassandra-driver-3.29.3 geomet-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f52ffc17-7f19-4db2-aedb-ee7aebd277e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Conectando a Cassandra...\n",
      "2. Iniciando ingesta de 100000 registros...\n",
      "   -> Insertados 10000 registros...\n",
      "   -> Insertados 20000 registros...\n",
      "   -> Insertados 30000 registros...\n",
      "   -> Insertados 40000 registros...\n",
      "   -> Insertados 50000 registros...\n",
      "   -> Insertados 60000 registros...\n",
      "   -> Insertados 70000 registros...\n",
      "   -> Insertados 80000 registros...\n",
      "   -> Insertados 90000 registros...\n",
      "   -> Insertados 100000 registros...\n",
      "------------------------------\n",
      "RESULTADO FASE 2 (INGESTA):\n",
      "Tiempo Total: 423.0338 segundos\n",
      "Velocidad: 236.39 inserts/seg\n",
      "------------------------------\n",
      "Registros en Cassandra: 100000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "import random\n",
    "from datetime import date, timedelta\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "# --- CONFIGURACIÓN ---\n",
    "# Conectamos al servicio 'cassandra' definido en docker-compose\n",
    "print(\"1. Conectando a Cassandra...\")\n",
    "cluster = Cluster(['cassandra']) \n",
    "session = cluster.connect()\n",
    "\n",
    "# Crear Keyspace y Tabla si no existen (por seguridad)\n",
    "session.execute(\"\"\"\n",
    "    CREATE KEYSPACE IF NOT EXISTS bigdata \n",
    "    WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\n",
    "\"\"\")\n",
    "session.set_keyspace('bigdata')\n",
    "\n",
    "session.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS ventas_crudas (\n",
    "        id_venta UUID,\n",
    "        fecha_venta DATE,\n",
    "        id_producto INT,\n",
    "        categoria TEXT,\n",
    "        monto_total FLOAT,\n",
    "        id_cliente INT,\n",
    "        PRIMARY KEY ((fecha_venta), id_venta)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Preparar la query de inserción para optimizar velocidad\n",
    "prepared = session.prepare(\"\"\"\n",
    "    INSERT INTO ventas_crudas (id_venta, fecha_venta, id_producto, categoria, monto_total, id_cliente)\n",
    "    VALUES (?, ?, ?, ?, ?, ?)\n",
    "\"\"\")\n",
    "\n",
    "# --- GENERADOR DE DATOS ---\n",
    "categorias = ['Electronica', 'Ropa', 'Hogar', 'Juguetes', 'Deportes','Electrodomesticos','Gaming','Vehiculos','Telefonos','Computacion']\n",
    "start_date = date(2026, 1, 1)\n",
    "\n",
    "def generar_lote(n):\n",
    "    datos = []\n",
    "    for _ in range(n):\n",
    "        datos.append((\n",
    "            uuid.uuid4(),\n",
    "            start_date + timedelta(days=random.randint(0, 30)),\n",
    "            random.randint(1, 100),\n",
    "            random.choice(categorias),\n",
    "            round(random.uniform(10.0, 500.0), 2),\n",
    "            random.randint(1000, 5000)\n",
    "        ))\n",
    "    return datos\n",
    "\n",
    "# --- EJECUCIÓN Y MEDICIÓN (Fase 4.3) ---\n",
    "CANTIDAD = 100000\n",
    "print(f\"2. Iniciando ingesta de {CANTIDAD} registros...\")\n",
    "\n",
    "inicio = time.time()\n",
    "\n",
    "# Insertamos en lotes pequeños para no saturar la memoria del notebook\n",
    "for i in range(CANTIDAD):\n",
    "    # Generamos datos al vuelo\n",
    "    d = (\n",
    "        uuid.uuid4(),\n",
    "        start_date + timedelta(days=random.randint(0, 30)),\n",
    "        random.randint(1, 100),\n",
    "        random.choice(categorias),\n",
    "        round(random.uniform(10.0, 500.0), 2),\n",
    "        random.randint(1000, 5000)\n",
    "    )\n",
    "    session.execute(prepared, d)\n",
    "    \n",
    "    if (i+1) % 10000 == 0:\n",
    "        print(f\"   -> Insertados {i+1} registros...\")\n",
    "\n",
    "fin = time.time()\n",
    "tiempo_ingesta = fin - inicio\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RESULTADO FASE 2 (INGESTA):\")\n",
    "print(f\"Tiempo Total: {tiempo_ingesta:.4f} segundos\")\n",
    "print(f\"Velocidad: {CANTIDAD/tiempo_ingesta:.2f} inserts/seg\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Validación simple (Fase 2.3)\n",
    "row = session.execute(\"SELECT COUNT(*) FROM ventas_crudas\").one()\n",
    "print(f\"Registros en Cassandra: {row[0]}\")\n",
    "\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b9cce92-19f1-4d37-ab87-523d1ece4d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Inicializado correctamente.\n",
      "1. Leyendo datos desde Cassandra (ventas_crudas)...\n",
      "2. Procesando/Transformando datos...\n",
      "+-----------+-----------------+-----------------+----------------------+\n",
      "|fecha_venta|        categoria|     total_ventas|cantidad_transacciones|\n",
      "+-----------+-----------------+-----------------+----------------------+\n",
      "| 2026-01-01|      Computacion|98283.70983600616|                   387|\n",
      "| 2026-01-01|         Deportes| 85681.6800327301|                   325|\n",
      "| 2026-01-01|Electrodomesticos|83727.56995487213|                   324|\n",
      "| 2026-01-01|      Electronica|80011.86010074615|                   328|\n",
      "| 2026-01-01|           Gaming|78706.82979297638|                   307|\n",
      "| 2026-01-01|            Hogar|83039.83976268768|                   323|\n",
      "| 2026-01-01|         Juguetes|81142.97006416321|                   321|\n",
      "| 2026-01-01|             Ropa|72797.10990810394|                   304|\n",
      "| 2026-01-01|        Telefonos|84619.85998058319|                   340|\n",
      "| 2026-01-01|        Vehiculos|86708.07987594604|                   351|\n",
      "+-----------+-----------------+-----------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "3. Escribiendo resultados en ClickHouse (dw_analitico)...\n",
      "¡Carga a Data Warehouse exitosa!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, count, col\n",
    "\n",
    "# --- CONFIGURACIÓN DE SPARK CON DEPENDENCIAS ---\n",
    "# Aquí definimos los paquetes Maven necesarios para Cassandra y ClickHouse\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoBigData_UNEG\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.4.1,ru.yandex.clickhouse:clickhouse-jdbc:0.3.2\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Inicializado correctamente.\")\n",
    "\n",
    "# --- FASE 3: LECTURA Y TRANSFORMACIÓN ---\n",
    "print(\"1. Leyendo datos desde Cassandra (ventas_crudas)...\")\n",
    "# Leemos la tabla completa\n",
    "df_raw = spark.read \\\n",
    "    .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "    .options(table=\"ventas_crudas\", keyspace=\"bigdata\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformación (Requisito 3.2): Agrupar por fecha y categoría\n",
    "print(\"2. Procesando/Transformando datos...\")\n",
    "df_analitico = df_raw.groupBy(\"fecha_venta\", \"categoria\") \\\n",
    "    .agg(\n",
    "        sum(\"monto_total\").alias(\"total_ventas\"),\n",
    "        count(\"id_venta\").alias(\"cantidad_transacciones\")\n",
    "    ) \\\n",
    "    .orderBy(\"fecha_venta\", \"categoria\")\n",
    "\n",
    "# Mostramos un adelanto para verificar\n",
    "df_analitico.show(10)\n",
    "\n",
    "# --- FASE 4.1: CARGA A CLICKHOUSE ---\n",
    "print(\"3. Escribiendo resultados en ClickHouse (dw_analitico)...\")\n",
    "\n",
    "# Propiedades JDBC para ClickHouse\n",
    "jdbc_url = \"jdbc:clickhouse://clickhouse:8123/dw_analitico\"\n",
    "propiedades = {\n",
    "    \"driver\": \"ru.yandex.clickhouse.ClickHouseDriver\",\n",
    "    \"user\": \"default\",\n",
    "    \"password\": \"\"  # Si no pusiste clave en docker-compose\n",
    "}\n",
    "\n",
    "# Escritura\n",
    "try:\n",
    "    df_analitico.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(url=jdbc_url, table=\"ventas_resumen\", properties=propiedades)\n",
    "    print(\"¡Carga a Data Warehouse exitosa!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error escribiendo a ClickHouse: {e}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678b3f5-0fc0-4e82-8a12-2715cca547c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
